"""Master class for running experiments"""

import logging
import time
import subprocess
import cPickle as pickle
from copy import deepcopy
import os
import psutil
import sys

import create_files
from lens import write_lens_files
from experiment import evaluate_experiment
from segmentation import test_segmentation


LENS_LOCATION = '/Applications/LensOSX.app/Contents/MacOS/LensOSX'
LENS_NAME = 'LensOSX'

if not os.path.isfile(LENS_LOCATION):
    LENS_LOCATION = '/Users/fred/Applications/LensOSX.app/Contents/MacOS/LensOSX'

logging.basicConfig(level=logging.WARNING)


class Network(object):
    """A simple recurrent network to be used in experimental modeling

    Attributes:
        seed (int): seed used for random initial net weights.
        hidden (int): number of hidden units.
        rate (float): learning rate.
        momentum (float): momentum.
        ticks (int): number of examples that backpropogation goes back.
        num_train (int): number of training examples.
        incoding (dict): maps phonemes to input layer network representations.
        outcoding (dict): maps phonemes to outupt layer network representations.
        time (str): last time the network was modified.
        segmentation_results (dict): network performance on the segmentation task.
            Generated by `self.train_network()`.
            Keys:
                boundary_precision (float): boundary detection precision
                boundary_recall (float): boundary detection recall
                boundary_F (float): boundary detection F score
                word_precision (float): word detection precision
                word_recall (float): word detection recall
                word_F (float): word detection F score
        experiment_results (dict): network performance on the ALL experiment.
            Generated by `self.run_experiment()`.
            Keys:
                exp(A|B)_accuracy (float): average accuracy of choices for the
                    (contoid|vocoid) language across 1000 trials
                exp(A|B)_std (float): standard deviation of accuracy
                exp(A|B)_errors (tuple list): network errors for each trial on
                    the (contoid|vocoid) language. Note that these are errors in the
                    neural network sense, not errors in the experiment
                exp(A|B)_rts (float list): reaction time for each trial
                    currently, the inverse of percentage difference between errors

        """
    def __init__(self, lang, seed=0, distributed=False, hidden=80, rate=0.1,
                 momentum=0.95, ticks=1, rand_range=0.25, num_train=696692,):
        super(Network, self).__init__()

        self.lang = lang
        self.seed = seed
        self.hidden = hidden
        self.rate = rate
        self.momentum = momentum
        self.ticks = ticks
        self.rand_range = rand_range
        self.num_train = num_train
        self.distributed = distributed
        self.incoding = {}
        self.outcoding = {}
        self.time = time.strftime('%m/%d at %H:%M:%S')  # last time results were changed
        self._id = None

        self.segmentation_results = {}
        self.experiment_results = {}

        # these methods defines critical attributes, thus
        # they must be called upon initialization
        self._write_example_files()
        self._create_id()

        # load previously trained and tested network if possible
        try:
            net = pickle.load(open('nets/%s.p' % self._id))
            self.segmentation_results = net.segmentation_results
            self.experiment_results = net.experiment_results
            self.time = net.time
            logging.info('loading existing network results')
        except IOError:
            pickle.dump(self, open('nets/%s.p' % self._id, 'wb'))

    def __getattr__(self, attr):
        # treat keys of results dicts as attributes
        if attr.startswith('__') and attr.endswith('__'):
            # workaround for bugs with pickle and special methods
            return super(Network, self).__getattr__(attr)
        try:
            return self.segmentation_results[attr]
        except KeyError:
            try:
                return self.experiment_results[attr]
            except:
                raise AttributeError("'Network' object has no attribute '%s'" % attr)

    def train_network(self, retrain=True):
        """Trains the network and tests segmentation ability.

        Updates self.segmentation_results with results.
        """
        logging.debug('executing train_network()')
        self._write_example_files()
        self._write_lens_files()
        trained = os.path.isfile('lens/weights/' + self._id + '.wt')
        if not trained or retrain:
            self._run_lens('training')
        self._run_lens('testing')
        self.segmentation_results = test_segmentation(self.lang, 'break_average')

        self.time = time.strftime('%m/%d at %H:%M:%S')
        pickle.dump(self, open('nets/%s.p' % self._id, 'wb'))

    def run_experiment(self):
        """Runs the network on the ALL experiment.

        Updates self.experiment_results with results.
        See experiment.py for details.
        """
        logging.debug('executing run_experiment()')
        self._write_example_files()
        self._write_lens_files()
        lens_output = self._run_lens('experiment')
        self.experiment_results = evaluate_experiment(lens_output)

        self.time = time.strftime('%m/%d at %H:%M:%S')
        pickle.dump(self, open('nets/%s.p' % self._id, 'wb'))

    def _create_id(self):
        """Create id which uniquly identifies self"""
        logging.debug('executing _create_id()')
        distributed = 'd' if self.distributed else 'l'
        rate = str(self.rate)[2:]
        momentum = str(self.momentum)[2:]
        rand_range = str(self.rand_range)[2:]
        self._id = ('%s_%s_%s_%s_%s_%s_%s_%s_%s'
                    % (self.lang, distributed, self.seed, self.hidden, rate,
                       momentum, self.ticks, rand_range, self.num_train,))

    def _write_example_files(self):
        logging.debug('executing _write_example_files()')
        # note: num_train is updated to the actual number of
        # examples that are generated
        tmp = self.num_train
        self.num_train, self.incoding, self.outcoding = \
            create_files.write_example_files(self.lang, self.distributed, self.num_train)
        if self.num_train != tmp:
            logging.warning('num_train has been changed from'
                            '%s to %s' % (tmp, self.num_train))

    def _write_lens_files(self):
        logging.debug('executing _write_lens_files()')
        input_ = len(self.incoding)
        output = len(self.outcoding)
        write_lens_files(self._id, self.seed, input_, self.hidden,
                         output, self.num_train, self.rate,
                         self.momentum, self.ticks, self.rand_range)

    def _run_lens(self, in_file):
        """Executes a lens .in file and returens output.

        Raises RuntimeError if there is an error in the lens script."""
        try:
            # todo: pipe output to terminal and python
            logging.info("executing _run_lens('%s')" % in_file)
            out = subprocess.Popen([LENS_LOCATION, '-b', 'lens/%s.in' % in_file],
                                   stdout=subprocess.PIPE).communicate()[0]
            if out[-8:] != 'success\n':  # script echos 'success' at the end
                raise RuntimeError('Error whil executing lens/%s.in' % in_file)
            out = out[:out.rindex('\n')]  # remove success message
            with open('%s-log.out' % in_file, 'w+') as f:
                f.write(out)
            return out
        finally:
            # kill any remaining Lens processes
            for proc in psutil.process_iter():
                if proc.name() == LENS_NAME:
                    proc.kill()


def run_nets(parameters, rerun=None):
    """Runs a series of nets."""
    if not isinstance(parameters, list):
        parameters = [parameters]
    print 'NOW RUNNING %s NETS' % len(parameters)
    for param in parameters:
        net = Network(**param)
        net.train_network()
        net.run_experiment()


def generate_permutations(parameters):
    """Returns [dict]: all permutations of params in parameters.

    parmeters must be a list of (str, list), where str is
    the key and list is a list of values"""

    def recurse(parameters, permutations):
        if not parameters:
            return permutations

        # return a copy of permutations with all possible values
        #   for param added to each permutation
        # this multiplies len(permutations) by len(values)
        param, values = parameters.pop(0)
        new_perms = []
        for v in values:
            perm_copy = deepcopy(permutations)
            for perm in perm_copy:
                perm[param] = v
            new_perms += perm_copy

        return recurse(parameters, new_perms)

    parameters.reverse()  # so that result is sorted by first parameter
    param, values = parameters.pop(0)
    permutations = [{param: val} for val in values]
    return recurse(parameters, permutations)


def main(*args):
    params = generate_permutations([('lang', ['danish', 'english']),
                                    ('seed', range(28)),
                                    ('distributed', [True, False])])
    run_nets(params)


if __name__ == '__main__':
    main(sys.argv[1:])
